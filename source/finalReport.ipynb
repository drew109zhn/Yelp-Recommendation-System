{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 0. Initial set up\n",
    "In this report, we will walk you through our learning process. At the same time, we will also include information about steps to run our code and visualize our results. \n",
    "\n",
    "First, please run the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Background\n",
    "We used data made public for educational purposes by Yelp Inc.. To motivate students and researchers in their pursuits of developments of better algorithms and practices in tackling big and complex data, Yelp created public challenges by publishing a part of their data and  challeng scholars and curious minds to find meanings in their wealth of data. The data we downloaded from Yelp is 4.98GB; we used data from round 9 challenge. The link to the data is: https://www.yelp.com/dataset_challenge\n",
    "\n",
    "\n",
    "## 1. Item-based recommendation system\n",
    "## 2. Our original recommendation method\n",
    "Our original recommendation system predicts businesses a user x will like based on x's based reviews. First, we feed our algorithm a list of x's past business reviews in a csv file. Each business review includes a list of binary attributes which indicates whether the business is in certain categories. For instance. McDonald might have a list of attributes such as \"fastFood\", \"shopping\" and \"spa\", so it will have a 1 for \"fastFood\" indicating it is a fast food resteurant and have 0 for both \"shopping\" and \"spa\". Each business also has a binary attribute \"like\" showing whether x likes the business or not. It is also the target attribute of our model. Yelp users can rate businesses from 1 to 5, with 5 being the best rating. We set a threshold of 4, so if x gives a 4 or 5 stars to a business, we say x likes it, so set the \"like\" attribute for the business to be 1, otherwise set it to be 0. \n",
    "\n",
    "Given a list of business reviews by x, we split the list into trainData and testData. We use the trainData to train on Decision Tree, Logistic Regression and Naive Bayes models. We then test each model's accuracy with our testData and write them to a file. Along the process we also calculate each model's precision score and recall score for comparison \n",
    "purpose later. Each user will be recorded as one row in the accuracy and precision_recall file and the format of accuracy file will be :\n",
    "\n",
    "userID\t               Decision Tree\tLogistic Regression\t Naive Bayes\n",
    "x\t                    54.7309833\t    56.95732839\t         43.41372913\n",
    "y                   \t62.06206206\t    66.36636637        \t 38.23823824\n",
    "...\n",
    "\n",
    "The format of precision_recall file will be:\n",
    "\n",
    "username\t           DT_precision\tDT_recall\tLR_precision\tLR_recall\tNB_precision\tNB_recall\n",
    "x                    \t0.375586854\t0.418848168\t0.262910798\t   0.427480916\t0.887323944\t    0.40212766\n",
    "y                    \t0.396969697\t0.421221865\t0.260606061\t   0.483146067\t0.857575758\t    0.331770223\n",
    "...\n",
    "\n",
    "After computing accuracy and precision_recall scores, we choose the model with the highest accuracy and use it to recommendate businesses. For instance, given the input to be a list of potential businesses and the model to be decision tree, we feed all the potential businesses to decision tree model and only output the ones the user x likes(\"like\" attribute is predicted to be 1). Each potential business also has a list of binary attributes as business reviews discussed above, but it does not have \"like\" attribute since we are predicting user preferences. At the end our system will generate a file containing all the businesses user x likes out of the potential businesses. The file will look like:\n",
    "\n",
    "McDonald\n",
    "Slices\n",
    "Frank\n",
    "...\n",
    "\n",
    "\n",
    "## 3. Testing the results\n",
    "# IV. Results\n",
    "# V. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Aims\n",
    "First, we want to do some data exploration to find interesting patterns from the data. Some interesting questions that we wanted to answer are:\n",
    "1, What are the times in a week where most people would check in to businesses on Yelp?\n",
    "2, How does the distribution of users' number of reviews look like?\n",
    "3, How does the distribution of users' ratings look like? \n",
    "4, What constitute businesses on Yelp?\n",
    "5, Are there any correlations between Yelp users' number of friends and their average ratings?\n",
    "\n",
    "Second, we decided to apply what we learned in this data science course to  create a system that recommends businesses to Yelp users. In particular, given that Yelp users may have reviewed businesses that they have experienced, we used the machine learning methods introduced in class to find out -- from a poll of potential businesses close to where the users are -- the businesses that they may like, and recommend those to the users. Our recommendation system has restrictions, which will be discussed later in our report.\n",
    "\n",
    "In order to evaluate our models, we also implemented a tradition recommendation method: Item- Based recommendation Algorithm. We did analysis to measure how much our models' recommendations results differ from the Item-Based alogrithm's results. We also analyzed the predictive power of our model and Item-Based model -- given our data-- by the conventional training-testing approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Methods\n",
    "## 1, Set up input data for models\n",
    "### 1.1, Obtain and organize data:\n",
    "- First, you have to download the Yelp data from https://www.yelp.com/dataset_challenge/dataset. You will find a compressed file in your Downloads forlder. Uncompress it (tar xvzf yelp_dataset_challenge_round9.tar), name it 'yelp_data', and move the resulting folder in the directory of this project (It has to be in the same directory as the source folder, i.e. not inside the source folder).\n",
    "\n",
    "- Second, install dependency for this project: \n",
    "        pip install psycopg2 \n",
    "        pip install simplejson\n",
    "\n",
    "- Third, create the Yelp data based from the json files downloaded from Yelp. Into your terminal, inside this project folder: \n",
    "        createdb yelp\n",
    "  cd into source folder, on your terminal: \n",
    "        psql yelp -f create_yelp_dataset.sql\n",
    "  The schema of this data base are all written inside create_yelp_dataset.sql. We made some modifications to the general structure of data base given from Yelp DUY CAN YOU WRITE THIS PART\n",
    "  Into the terminal: \n",
    "      python populate_db.py\n",
    "  We do not make the code live in this report (functions inside populate_db are care called from this report), because this code should be run once only, and you should not make a mistake of running it again. The time it takes to import data from json files to local database is 40 mins- 60 mins. \n",
    "  \n",
    "- (Optional) If you prefer to deal with .csv files, you can convert the json files to csv files\n",
    "        \n",
    "        python json_to_csv_converter.py json_file_name\n",
    "   This actually does not help in anything in our project, except wasting your time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2, Set up input files\n",
    "Our implementation requires reading and writing to files to communicate between differnt processes of the algorithms. \n",
    "First, create necessary folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up all necessary directories\n",
    "import setup_io\n",
    "setup_io.create_output_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside our project folder, we created:\n",
    "- ./output : Where all outputs are stored\n",
    "- ./output/users: We need to find users' id that have reviewed a certain number of businesses in order to run our recommendation system. All these users ids will be quieried and ordered in order of descending users' number of reviews. The ids' are then written into files inside this folder\n",
    "- ./output/original: Our algorithm requires that given an user's id, we have to extract information about attributes of all busnissess that the user experienced, and whether the user rated this business positively or negatively. Each user's data will split into training data and test data. All files used to run our original system will be stored inside this folder. Details of each file are explained later.\n",
    "- ./output/item_based: Item- based recommendation algorithm requires inputs of user_id, businesses_id and users' ratings of the busineeses. We also divide each user's data into train data file and test data file. All files necessary to run this algorithm are stored inside this folder. Details of each file will be explained later.\n",
    "- ./output/potentials: For each user, we queried businesses' id in the same cities as the businesses that the user has reviewed. These businesses are potential recommendations for the users. The two recommendation systems will predict -- from this pool of potential recommendations -- the businesses that the user will more likely to love. Data of potential recommendations, and cities that the users have been to, are all stored inside this folder. Files details will be explained later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3, Find user_id of users who we want to run the recommendation system on\n",
    "As we explained above, our model will take data of previous ratings of a particular user, and then learn from those data to decide whether that user will like any other restaurants. The more data we have about an users' previous opinions about businesses, the better we can learn about that user's preferences. Our model may not work for users who have not made any reviews of businesses before. Therefore, before we apply our model, we have to find out lists of users who have reviewed a significant number of businesses. We querried users and ordered them by descending number of reviews. We offer functions to select user_ids in 2 ways: \n",
    "- We select users who has reviewed more than a certain number of times. Call the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import get_ideal_users as users\n",
    "import macros as m\n",
    "# Get user id that have more than a certain threshold number of reviews\n",
    "threshold = 1000\n",
    "fname = m.get_user_more_thres_fname(threshold)\n",
    "users.get_more_threshold_users(threshold, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 9 users who have reviewed more than 1000 businesses. \n",
    "The above function will write all validate user_ids into file './output/users/users_more_<theshold>.txt'. Each user_id is in its own line.\n",
    "- We can also select a specified number of users in order of descending number of reviews. Call: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import get_ideal_users as users\n",
    "import macros as m\n",
    "num_users = 100 # We get 100 user_id with the highest number of reviews\n",
    "fname = m.get_user_limit_fname(num_users) # get the file name to store ideal user_id\n",
    "users.get_users_limit(num_users, fname) # Get the 100 people with highest number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with user 0\n",
      "Done with user 1\n",
      "Done with user 2\n",
      "Done with user 3\n",
      "Done with user 4\n",
      "Done with user 5\n",
      "Done with user 6\n",
      "Done with user 7\n",
      "Done with user 8\n"
     ]
    }
   ],
   "source": [
    "import get_ideal_users as users\n",
    "import macros as m\n",
    "import get_reviews_attributes as process_input\n",
    "# Get the first x users that have the most reviews\n",
    "#num_users = 100 # Write your desired number of users to run analysis here, default is 100\n",
    "fname = '../output/users/users_more_1000.txt'#m.get_user_limit_fname(num_users)\n",
    "#users.get_users_limit(num_users, fname)\n",
    "#training percent: 70\n",
    "process_input.process_all_user_input(fname, 70, \\\n",
    "                                    m.default_star_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4, Find restaurants attributes\n",
    "Given an user_id, we have to find attributes of restaurants that the users reviewed.\n",
    "- We querried all attributes (what kind of parking lot?, free wifi?, etc.) and categories (Chinese food?, Italian food?, nail spa?, etc.) of restaurants that the user went to.  These attributes are stored in arrays of variable length inside the database, some elements of the array are arrays themselves, some aren't. We wrote raw data into file ./output/original/businesses_<userID>.txt . \n",
    "- From this raw input file, we used string methods to turn raw data of businesss features into binary data. The resulting data is written into files ./output/original/att_cat_<userID>_train.txt and ./output/original/att_cat_<userID>_test.txt. In order to do this, we processed raw data file to extract real restaurant features (For example, 'Free Wifi: False' is a real feature, \"priceRange: {'1: True', '2: False', '3: False'}\", then 'priceRange_1' is included in the list of attributes). We collected a set of attributes of all businesses; all attributes are written in comma-separated format on the first line of the file. For each business, we wrote its features into binary format based on the set of all attributes that we collected earlier.\n",
    "- The last attribute that we write into this file (./output/original/att_cat_<userID>_train/test.txt) is whether or not the user like the bus or not. To keep thing simple, we decided that if the user rated the restaurant more than 4 stars, the user like the business.\n",
    "\n",
    "Code for this process of data cleaning is written in get_reviews_attributes.py.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the two files of train and test data for our original methods of using traditional machine learning methods, we can run the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import get_reviews_attributes as process_input\n",
    "process_input.process_one_user_input('CxDOIDnH8gp9KXzpBHJYXw', 70, m.default_star_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, parameters include: \n",
    "(1) the id of the user that we want to create train and test data of businesses attributes that the user has been to\n",
    "(2) The percentage of training data, out of all data . We always use 70% of our data for training and 30% for testing\n",
    "(3) Star_threshold dictates how we decide whether an user like a business or not. We choose the default_star_threshold to be 4. Any businesses receiving ratings higher than this threshold are considered liked by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to create such files and plots to demonstrate the most popular check-in time during the week for\\\n",
    "# individual cities\n",
    "import get_popular_time\n",
    "# create a bunch of files that record the time adn average check-ins time for each city, but we only record\\\n",
    "# 10 cities with the most number of check-ins\n",
    "get_popular_time.create_popular_time_cities()\n",
    "get_popular_time.plot_popular_time_all_cities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import restaurant_location_ratings\n",
    "#restaurant_location_ratings.create_res_loc_ratings_by_city()\n",
    "#restaurant_location_ratings.plot_res_loc_ratings_by_city()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import visualize_friends_stars_users as f_r\n",
    "f_r.get_friends_ratings_friends()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}