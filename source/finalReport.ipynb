{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 0. Initial set up\n",
    "In this report, we will walk you through our learning process. At the same time, we will also include information about steps to run our code and visualize our results. \n",
    "\n",
    "First, please run the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Background\n",
    "We used data made public for educational purposes by Yelp Inc.. To motivate students and researchers in their pursuits of developments of better algorithms and practices in tackling big and complex data, Yelp created public challenges by publishing a part of their data and  challeng scholars and curious minds to find meanings in their wealth of data. The data we downloaded from Yelp is 4.98GB; we used data from round 9 challenge. The link to the data is: https://www.yelp.com/dataset_challenge\n",
    "\n",
    "\n",
    "## 1. Item-based recommendation system\n",
    "## 2. Our original recommendation method\n",
    "Our original recommendation system predicts businesses a user x will like based on x's based reviews. First, we feed our algorithm a list of x's past business reviews in a csv file. Each business review includes a list of binary attributes which indicates whether the business is in certain categories. For instance. McDonald might have a list of attributes such as \"fastFood\", \"shopping\" and \"spa\", so it will have a 1 for \"fastFood\" indicating it is a fast food resteurant and have 0 for both \"shopping\" and \"spa\". Each business also has a binary attribute \"like\" showing whether x likes the business or not. It is also the target attribute of our model. Yelp users can rate businesses from 1 to 5, with 5 being the best rating. We set a threshold of 4, so if x gives a 4 or 5 stars to a business, we say x likes it, so set the \"like\" attribute for the business to be 1, otherwise set it to be 0. \n",
    "\n",
    "Given a list of business reviews by x, we split the list into trainData and testData. We use the trainData to train on Decision Tree, Logistic Regression and Naive Bayes models. We then test each model's accuracy with our testData and write them to a file. Along the process we also calculate each model's precision score and recall score for comparison \n",
    "purpose later. Each user will be recorded as one row in the accuracy and precision_recall file and the format of accuracy file will be :\n",
    "\n",
    "\n",
    "| userID        | Decision Tree           | Logistic Regression  |Naive Bayes\n",
    "| ------------- |:-------------:| -----:| ---:|\n",
    "|x    | 54.7309833 | 56.95732839\t |43.41372913|\n",
    "|y     | 62.06206206|   66.36636637 |38.23823824|\n",
    "|... | ...      |    ... |...|\n",
    "...\n",
    "\n",
    "The format of precision_recall file will be:\n",
    "\n",
    "\n",
    "username\t           DT_precision\tDT_recall\tLR_precision\tLR_recall\tNB_precision\tNB_recall\n",
    "x                    \t0.375586854\t0.418848168\t0.262910798\t   0.427480916\t0.887323944\t    0.40212766\n",
    "y                    \t0.396969697\t0.421221865\t0.260606061\t   0.483146067\t0.857575758\t   0.857575758\t\n",
    "...\n",
    "\n",
    "| userID        | DT_precision           | DT_recall  |LR_precision|LR_recall|NB_precision|NB_recall|\n",
    "| ------------- |:-------------:| -----:| ---:|------:|----:|-----:|\n",
    "|x    | 0.375586854| 0.418848168\t |0.262910798|0.427480916|0.887323944\t|0.40212766|\n",
    "|y     | 0.396969697| 0.421221865 |0.260606061|0.483146067|0.857575758\t|0.857575758\t\n",
    "|... | ...      |    ... |...|...|...|...|\n",
    "\n",
    "After computing accuracy and precision_recall scores, we choose the model with the highest accuracy and use it to recommendate businesses. For instance, given the input to be a list of potential businesses and the model to be decision tree, we feed all the potential businesses to decision tree model and only output the ones the user x likes(\"like\" attribute is predicted to be 1). Each potential business also has a list of binary attributes as business reviews discussed above, but it does not have \"like\" attribute since we are predicting user preferences. At the end our system will generate a file containing all the businesses user x likes out of the potential businesses. The file will look like:\n",
    "\n",
    "McDonald\n",
    "Slices\n",
    "Frank\n",
    "...\n",
    "\n",
    "\n",
    "## 3. Testing the results\n",
    "# IV. Results\n",
    "# V. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Aims\n",
    "First, we want to do some data exploration to find interesting patterns from the data. Some interesting questions that we wanted to answer are:\n",
    "1, What are the times in a week where most people would check in to businesses on Yelp?\n",
    "2, How does the distribution of users' number of reviews look like?\n",
    "3, How does the distribution of users' ratings look like? \n",
    "4, What constitute businesses on Yelp?\n",
    "5, Are there any correlations between Yelp users' number of friends and their average ratings?\n",
    "\n",
    "Second, we decided to apply what we learned in this data science course to  create a system that recommends businesses to Yelp users. In particular, given that Yelp users may have reviewed businesses that they have experienced, we used the machine learning methods introduced in class to find out -- from a poll of potential businesses close to where the users are -- the businesses that they may like, and recommend those to the users. Our recommendation system has restrictions, which will be discussed later in our report.\n",
    "\n",
    "In order to evaluate our models, we also implemented a tradition recommendation method: Item- Based recommendation Algorithm. We did analysis to measure how much our models' recommendations results differ from the Item-Based alogrithm's results. We also analyzed the predictive power of our model and Item-Based model -- given our data-- by the conventional training-testing approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Methods\n",
    "## 1, Set up input data for models\n",
    "### 1.1, Obtain and organize data:\n",
    "- First, you have to download the Yelp data from https://www.yelp.com/dataset_challenge/dataset. You will find a compressed file in your Downloads forlder. Uncompress it (tar xvzf yelp_dataset_challenge_round9.tar), name it 'yelp_data', and move the resulting folder in the directory of this project (It has to be in the same directory as the source folder, i.e. not inside the source folder).\n",
    "\n",
    "- Second, install dependency for this project: \n",
    "        pip install psycopg2 \n",
    "        pip install simplejson\n",
    "\n",
    "- Third, create the Yelp data based from the json files downloaded from Yelp. Into your terminal, inside this project folder: \n",
    "        createdb yelp\n",
    "  cd into source folder, on your terminal: \n",
    "        psql yelp -f create_yelp_dataset.sql\n",
    "  The schema of this data base are all written inside create_yelp_dataset.sql. We made some modifications to the general structure of data base given from Yelp DUY CAN YOU WRITE THIS PART\n",
    "  Into the terminal: \n",
    "      python populate_db.py\n",
    "  We do not make the code live in this report (functions inside populate_db are care called from this report), because this code should be run once only, and you should not make a mistake of running it again. The time it takes to import data from json files to local database is 40 mins- 60 mins. \n",
    "  \n",
    "- (Optional) If you prefer to deal with .csv files, you can convert the json files to csv files\n",
    "        \n",
    "        python json_to_csv_converter.py json_file_name\n",
    "   This actually does not help in anything in our project, except wasting your time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2, Set up input files\n",
    "Our implementation requires reading and writing to files to communicate between differnt processes of the algorithms. \n",
    "First, create necessary folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folders if necessary... Done\n"
     ]
    }
   ],
   "source": [
    "# Set up all necessary directories\n",
    "import setup_io\n",
    "setup_io.create_output_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside our project folder, we created:\n",
    "- ./output : Where all outputs are stored\n",
    "- ./output/users: We need to find users' id that have reviewed a certain number of businesses in order to run our recommendation system. All these users ids will be quieried and ordered in order of descending users' number of reviews. The ids' are then written into files inside this folder\n",
    "- ./output/original: Our algorithm requires that given an user's id, we have to extract information about attributes of all busnissess that the user experienced, and whether the user rated this business positively or negatively. Each user's data will split into training data and test data. All files used to run our original system will be stored inside this folder. Details of each file are explained later.\n",
    "- ./output/item_based: Item- based recommendation algorithm requires inputs of user_id, businesses_id and users' ratings of the busineeses. We also divide each user's data into train data file and test data file. All files necessary to run this algorithm are stored inside this folder. Details of each file will be explained later.\n",
    "- ./output/potentials: For each user, we queried businesses' id in the same cities as the businesses that the user has reviewed. These businesses are potential recommendations for the users. The two recommendation systems will predict -- from this pool of potential recommendations -- the businesses that the user will more likely to love. Data of potential recommendations, and cities that the users have been to, are all stored inside this folder. Files details will be explained later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3, Find user_id of users who we want to run the recommendation system on\n",
    "As we explained above, our model will take data of previous ratings of a particular user, and then learn from those data to decide whether that user will like any other restaurants. The more data we have about an users' previous opinions about businesses, the better we can learn about that user's preferences. Our model may not work for users who have not made any reviews of businesses before. Therefore, before we apply our model, we have to find out lists of users who have reviewed a significant number of businesses. We querried users and ordered them by descending number of reviews. We offer functions to select user_ids in 2 ways: \n",
    "- We select users who has reviewed more than a certain number of times. Call the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import get_ideal_users as users\n",
    "import macros as m\n",
    "# Get user id that have more than a certain threshold number of reviews\n",
    "threshold = 1000\n",
    "fname = m.get_user_more_thres_fname(threshold)\n",
    "users.get_more_threshold_users(threshold, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 9 users who have reviewed more than 1000 businesses. \n",
    "The above function will write all validate user_ids into file './output/users/users_more_<theshold>.txt'. Each user_id is in its own line.\n",
    "- We can also select a specified number of users in order of descending number of reviews. Call: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import get_ideal_users as users\n",
    "import macros as m\n",
    "num_users = 100 # We get 100 user_id with the highest number of reviews\n",
    "fname = m.get_user_limit_fname(num_users) # get the file name to store ideal user_id\n",
    "users.get_users_limit(num_users, fname) # Get the 100 people with highest number of reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4, Find restaurants attributes\n",
    "Given an user_id, we have to find attributes of restaurants that the users reviewed.\n",
    "- We querried all attributes (what kind of parking lot?, free wifi?, etc.) and categories (Chinese food?, Italian food?, nail spa?, etc.) of restaurants that the user went to.  These attributes are stored in arrays of variable length inside the database, some elements of the array are arrays themselves, some aren't. We wrote raw data into file ./output/original/businesses_<userID>.txt . \n",
    "- From this raw input file, we used string methods to turn raw data of businesss features into binary data. The resulting data is written into files ./output/original/att_cat_<userID>_train.txt and ./output/original/att_cat_<userID>_test.txt. In order to do this, we processed raw data file to extract real restaurant features (For example, 'Free Wifi: False' is a real feature, \"priceRange: {'1: True', '2: False', '3: False'}\", then 'priceRange_1' is included in the list of attributes). We collected a set of attributes of all businesses; all attributes are written in comma-separated format on the first line of the file. For each business, we wrote its features into binary format based on the set of all attributes that we collected earlier.\n",
    "- The last attribute that we write into this file (./output/original/att_cat_<userID>_train/test.txt) is whether or not the user like the bus or not. To keep thing simple, we decided that if the user rated the restaurant more than 4 stars, the user like the business.\n",
    "\n",
    "Code for this process of data cleaning is written in get_reviews_attributes.py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with user 0\n",
      "Done with user 1\n",
      "Done with user 2\n",
      "Done with user 3\n",
      "Done with user 4\n",
      "Done with user 5\n",
      "Done with user 6\n",
      "Done with user 7\n",
      "Done with user 8\n",
      "Done with user 9\n",
      "Done with user 10\n",
      "Done with user 11\n",
      "Done with user 12\n",
      "Done with user 13\n",
      "Done with user 14\n",
      "Done with user 15\n",
      "Done with user 16\n",
      "Done with user 17\n",
      "Done with user 18\n",
      "Done with user 19\n",
      "Done with user 20\n",
      "Done with user 21\n",
      "Done with user 22\n",
      "Done with user 23\n",
      "Done with user 24\n",
      "Done with user 25\n",
      "Done with user 26\n",
      "Done with user 27\n",
      "Done with user 28\n",
      "Done with user 29\n",
      "Done with user 30\n",
      "Done with user 31\n",
      "Done with user 32\n",
      "Done with user 33\n",
      "There were problems executing the command SELECT bus_id, \t\t\t\t  COALESCE (attributes, NULL) as attributes, COALESCE (categories, NULL) as categories                   FROM businesses                   WHERE  city = 'L'Île-Perrot';\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fe491c724ffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# fname = m.get_user_more_thres_fname(threshold)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# users.get_more_threshold_users(threshold, fname)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprocess_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_all_user_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/vagrant/project-duy-ha-pj/source/get_reviews_attributes.pyc\u001b[0m in \u001b[0;36mprocess_all_user_input\u001b[0;34m(user_fname, train_percent, star_threshold)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0muser_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mget_businesses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mbus_ib_exclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_one_user_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_percent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstar_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0muser_bus_ib_test_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbus_ib_exclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Done with user \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vagrant/project-duy-ha-pj/source/get_reviews_attributes.pyc\u001b[0m in \u001b[0;36mprocess_one_user_input\u001b[0;34m(user_id, train_percent, star_threshold)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0mcities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbusinesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;31m# 5. Write the cities the user has been to, adn all the businesses in those cities into file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mcreate_potentials_one_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitem_based_bus_exclude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;31m#print \"Done writing train and test data for item_based method\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vagrant/project-duy-ha-pj/source/get_reviews_attributes.pyc\u001b[0m in \u001b[0;36mcreate_potentials_one_user\u001b[0;34m(pot_features_set, cities, user_id)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \"\"\"\n\u001b[1;32m    330\u001b[0m     \u001b[0mraw_potential_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dir_potential\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/raw_bus_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muser_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0mwrite_raw_potential_recommendations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_potential_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m     \u001b[0mpot_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpot_businesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_potential_recommendations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_potential_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0mprocessed_pot_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dir_potential\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/pot_bus_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muser_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vagrant/project-duy-ha-pj/source/get_reviews_attributes.pyc\u001b[0m in \u001b[0;36mwrite_raw_potential_recommendations\u001b[0;34m(cities, fname)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"There were problems executing the command \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PSQL Execution problem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;31m# Write all businesses, their attributes and categories into file businesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_record\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_cities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "import get_ideal_users as users\n",
    "import macros as m\n",
    "import get_reviews_attributes as process_input\n",
    "# Get the first x users that have the most reviews\n",
    "num_users = 100 # Write your desired number of users to run analysis here, default is 100\n",
    "fname = m.get_user_limit_fname(num_users)\n",
    "users.get_users_limit(num_users, fname)\n",
    "#training percent: 70\n",
    "threshold = 1000\n",
    "# fname = m.get_user_more_thres_fname(threshold)\n",
    "# users.get_more_threshold_users(threshold, fname)\n",
    "process_input.process_all_user_input(fname, 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2, Call our original recommendation method\n",
    "After we call ml.run(fname), the algorithm first goes to the fname path and read a file containing all user's ID\n",
    "\n",
    "Second, it goes to the original folder and gets each user's business review trainData file and testData file.\n",
    "\n",
    "Third, for user's trainData file, the algorithm extracts the traindata and trains this data on Decision Tree, Logistic Regression and NaiveBayes models.\n",
    "\n",
    "Fourth, for user's testData file, the algorithm extracts the testData and tests each model's accuracy and precision and recall scores. The algorithm also writes these results into two separate files as original_accuracy.csv and original_precision_recall.csv and store them in the results folder. \n",
    "\n",
    "Fifth, the algorithm will go to the potentials folder and get a list of business potentials. It then picks the model with the highest accuracy and outputs a file of recommended businesses out of the potentials for each user.(**Writing each recommendation list can take as long as 1 minute depending on the file size**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "finished running models for the user ()CxDOIDnH8gp9KXzpBHJYXw\n",
      "finished writing CxDOIDnH8gp9KXzpBHJYXw into accuracy and precision_recall files\n",
      "finished writing CxDOIDnH8gp9KXzpBHJYXw's recommendation list\n",
      "finished running models for the user ()bLbSNkLggFnqwNNzzq-Ijw\n",
      "finished writing bLbSNkLggFnqwNNzzq-Ijw into accuracy and precision_recall files\n",
      "finished writing bLbSNkLggFnqwNNzzq-Ijw's recommendation list\n",
      "finished running models for the user ()PKEzKWv_FktMm2mGPjwd0Q\n",
      "finished writing PKEzKWv_FktMm2mGPjwd0Q into accuracy and precision_recall files\n",
      "finished writing PKEzKWv_FktMm2mGPjwd0Q's recommendation list\n",
      "finished running models for the user ()QJI9OSEn6ujRCtrX06vs1w\n",
      "finished writing QJI9OSEn6ujRCtrX06vs1w into accuracy and precision_recall files\n",
      "finished writing QJI9OSEn6ujRCtrX06vs1w's recommendation list\n",
      "finished running models for the user ()DK57YibC5ShBmqQl97CKog\n",
      "finished writing DK57YibC5ShBmqQl97CKog into accuracy and precision_recall files\n",
      "finished writing DK57YibC5ShBmqQl97CKog's recommendation list\n",
      "finished running models for the user ()d_TBs6J3twMy9GChqUEXkg\n",
      "finished writing d_TBs6J3twMy9GChqUEXkg into accuracy and precision_recall files\n",
      "finished writing d_TBs6J3twMy9GChqUEXkg's recommendation list\n",
      "finished running models for the user ()UYcmGbelzRa0Q6JqzLoguw\n",
      "finished writing UYcmGbelzRa0Q6JqzLoguw into accuracy and precision_recall files\n",
      "finished writing UYcmGbelzRa0Q6JqzLoguw's recommendation list\n",
      "finished running models for the user ()ELcQDlf69kb-ihJfxZyL0A\n",
      "finished writing ELcQDlf69kb-ihJfxZyL0A into accuracy and precision_recall files\n",
      "finished writing ELcQDlf69kb-ihJfxZyL0A's recommendation list\n",
      "finished running models for the user ()U4INQZOPSUaj8hMjLlZ3KA\n",
      "finished writing U4INQZOPSUaj8hMjLlZ3KA into accuracy and precision_recall files\n",
      "finished writing U4INQZOPSUaj8hMjLlZ3KA's recommendation list\n",
      "finished running models for the user ()hWDybu_KvYLSdEFzGrniTw\n",
      "finished writing hWDybu_KvYLSdEFzGrniTw into accuracy and precision_recall files\n",
      "finished writing hWDybu_KvYLSdEFzGrniTw's recommendation list\n",
      "finished running models for the user ()cMEtAiW60I5wE_vLfTxoJQ\n",
      "finished writing cMEtAiW60I5wE_vLfTxoJQ into accuracy and precision_recall files\n",
      "finished writing cMEtAiW60I5wE_vLfTxoJQ's recommendation list\n",
      "finished running models for the user ()62GNFh5FySkA3MbrQmnqvg\n",
      "finished writing 62GNFh5FySkA3MbrQmnqvg into accuracy and precision_recall files\n",
      "finished writing 62GNFh5FySkA3MbrQmnqvg's recommendation list\n",
      "finished running models for the user ()n86B7IkbU20AkxlFX_5aew\n",
      "finished writing n86B7IkbU20AkxlFX_5aew into accuracy and precision_recall files\n",
      "finished writing n86B7IkbU20AkxlFX_5aew's recommendation list\n",
      "finished running models for the user ()dIIKEfOgo0KqUfGQvGikPg\n",
      "finished writing dIIKEfOgo0KqUfGQvGikPg into accuracy and precision_recall files\n",
      "finished writing dIIKEfOgo0KqUfGQvGikPg's recommendation list\n",
      "finished running models for the user ()N3oNEwh0qgPqPP3Em6wJXw\n",
      "finished writing N3oNEwh0qgPqPP3Em6wJXw into accuracy and precision_recall files\n",
      "finished writing N3oNEwh0qgPqPP3Em6wJXw's recommendation list\n",
      "finished running models for the user ()rCWrxuRC8_pfagpchtHp6A\n",
      "finished writing rCWrxuRC8_pfagpchtHp6A into accuracy and precision_recall files\n",
      "finished writing rCWrxuRC8_pfagpchtHp6A's recommendation list\n",
      "finished running models for the user ()iDlkZO2iILS8Jwfdy7DP9A\n",
      "finished writing iDlkZO2iILS8Jwfdy7DP9A into accuracy and precision_recall files\n",
      "finished writing iDlkZO2iILS8Jwfdy7DP9A's recommendation list\n",
      "finished running models for the user ()0BBUmH7Krcax1RZgbH4fSA\n",
      "finished writing 0BBUmH7Krcax1RZgbH4fSA into accuracy and precision_recall files\n",
      "finished writing 0BBUmH7Krcax1RZgbH4fSA's recommendation list\n",
      "finished running models for the user ()Ry1O_KXZHGRI8g5zBR3IcQ\n",
      "finished writing Ry1O_KXZHGRI8g5zBR3IcQ into accuracy and precision_recall files\n",
      "finished writing Ry1O_KXZHGRI8g5zBR3IcQ's recommendation list\n",
      "finished running models for the user ()3nDUQBjKyVor5wV0reJChg\n",
      "finished writing 3nDUQBjKyVor5wV0reJChg into accuracy and precision_recall files\n",
      "finished writing 3nDUQBjKyVor5wV0reJChg's recommendation list\n",
      "finished running models for the user ()YMgZqBUAddmFErxLtCfK_w\n",
      "finished writing YMgZqBUAddmFErxLtCfK_w into accuracy and precision_recall files\n",
      "finished writing YMgZqBUAddmFErxLtCfK_w's recommendation list\n",
      "finished running models for the user ()pMefTWo6gMdx8WhYSA2u3w\n",
      "finished writing pMefTWo6gMdx8WhYSA2u3w into accuracy and precision_recall files\n",
      "finished writing pMefTWo6gMdx8WhYSA2u3w's recommendation list\n",
      "finished running models for the user ()dt9IHwfuZs9D9LOH7gjNew\n",
      "finished writing dt9IHwfuZs9D9LOH7gjNew into accuracy and precision_recall files\n",
      "finished writing dt9IHwfuZs9D9LOH7gjNew's recommendation list\n",
      "finished running models for the user ()PeLGa5vUR8_mcsn-fn42Jg\n",
      "finished writing PeLGa5vUR8_mcsn-fn42Jg into accuracy and precision_recall files\n",
      "finished writing PeLGa5vUR8_mcsn-fn42Jg's recommendation list\n",
      "finished running models for the user ()RBZ_kMjowV0t6_nv2UKaDQ\n",
      "finished writing RBZ_kMjowV0t6_nv2UKaDQ into accuracy and precision_recall files\n",
      "finished writing RBZ_kMjowV0t6_nv2UKaDQ's recommendation list\n",
      "finished running models for the user ()Wu0yySWcHQ5tZ_59HNiamg\n",
      "finished writing Wu0yySWcHQ5tZ_59HNiamg into accuracy and precision_recall files\n",
      "finished writing Wu0yySWcHQ5tZ_59HNiamg's recommendation list\n",
      "finished running models for the user ()qewG3X2O4X6JKskxyyqFwQ\n",
      "finished writing qewG3X2O4X6JKskxyyqFwQ into accuracy and precision_recall files\n",
      "finished writing qewG3X2O4X6JKskxyyqFwQ's recommendation list\n",
      "finished running models for the user ()SlgpAnj2gQd44EM_Uq6DkQ\n",
      "finished writing SlgpAnj2gQd44EM_Uq6DkQ into accuracy and precision_recall files\n",
      "finished writing SlgpAnj2gQd44EM_Uq6DkQ's recommendation list\n",
      "finished running models for the user ()ffPY_bHX8vLebHu8LBEqfg\n",
      "finished writing ffPY_bHX8vLebHu8LBEqfg into accuracy and precision_recall files\n",
      "finished writing ffPY_bHX8vLebHu8LBEqfg's recommendation list\n",
      "finished running models for the user ()fiGqQ7pIGKyZ9G0RqWLMpg\n",
      "finished writing fiGqQ7pIGKyZ9G0RqWLMpg into accuracy and precision_recall files\n",
      "finished writing fiGqQ7pIGKyZ9G0RqWLMpg's recommendation list\n",
      "finished running models for the user ()HJj82f-csBI7jjgenwqhvw\n",
      "finished writing HJj82f-csBI7jjgenwqhvw into accuracy and precision_recall files\n",
      "finished writing HJj82f-csBI7jjgenwqhvw's recommendation list\n",
      "finished running models for the user ()Q9mA60HnY87C1TW5kjAZ6Q\n",
      "finished writing Q9mA60HnY87C1TW5kjAZ6Q into accuracy and precision_recall files\n",
      "finished writing Q9mA60HnY87C1TW5kjAZ6Q's recommendation list\n",
      "finished running models for the user ()WeVkkF5L39888IPPlRhNpg\n",
      "finished writing WeVkkF5L39888IPPlRhNpg into accuracy and precision_recall files\n",
      "finished writing WeVkkF5L39888IPPlRhNpg's recommendation list\n",
      "finished running models for the user ()Xxvz5g67eaCr3emnkY5M6w\n",
      "finished writing Xxvz5g67eaCr3emnkY5M6w into accuracy and precision_recall files\n",
      "finished writing Xxvz5g67eaCr3emnkY5M6w's recommendation list\n",
      "finished running models for the user ()Wc5L6iuvSNF5WGBlqIO8nw\n",
      "finished writing Wc5L6iuvSNF5WGBlqIO8nw into accuracy and precision_recall files\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../output/potentials/pot_bus_Wc5L6iuvSNF5WGBlqIO8nw.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ae9430905fb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplyML\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/vagrant/project-duy-ha-pj/source/applyML.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(user_fname)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                     \u001b[0mfinalCLF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclfNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dir_results\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/original_result_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrecomOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dir_potential\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/pot_bus_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rd'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpotentials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                     \u001b[0mrecomWriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecomOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'bus_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0mrecomWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../output/potentials/pot_bus_Wc5L6iuvSNF5WGBlqIO8nw.txt'"
     ]
    }
   ],
   "source": [
    "import applyML as ml\n",
    "ml.run(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import get_reviews_attributes as process_input\n",
    "import macros as m\n",
    "process_input.get_businesses('DK57YibC5ShBmqQl97CKog')\n",
    "bus_ib_exclude = process_input.process_one_user_input('DK57YibC5ShBmqQl97CKog', 70, m.default_star_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, parameters include: \n",
    "(1) the id of the user that we want to create train and test data of businesses attributes that the user has been to\n",
    "(2) The percentage of training data, out of all data . We always use 70% of our data for training and 30% for testing\n",
    "(3) Star_threshold dictates how we decide whether an user like a business or not. We choose the default_star_threshold to be 4. Any businesses receiving ratings higher than this threshold are considered liked by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Some visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import restaurant_location_ratings\n",
    "#restaurant_location_ratings.create_res_loc_ratings_by_city()\n",
    "#restaurant_location_ratings.plot_res_loc_ratings_by_city()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1, Users' number of friends vs their ratings\n",
    "We want to investigate whether there are any relationships between users' number of friends and their average ratings. To do this, we querried users' number of friends (length of friends array inside users table in the database), and their average ratings. We then created a scatter plot to demonstrate the potential relationship. Call the following piece of code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import visualize_friends_stars_users as f_r\n",
    "f_r.get_friends_ratings_friends()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2, Popular check-ins time throughout the week\n",
    "We wanted to investigate the time during the week when users check in most often in each city. In order to do this, we:\n",
    "(1) Find cities with the most number of check-ins in all businesses existent in the cities. We hard coded our code so that we will only find the 10 cities with the most number of total check-ins. Function get_popular_time.py/ get_cities() will querry the database and find out these cities.\n",
    "(2) For each city, we find the average number of check-in in each hours of the week per restaurant (total number of check-ins during an hour in a city / number of businesses in that city). This information is printed out into \"../output/check_in_time_by_cities/<city_name>_check_ins.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to create such files and plots to demonstrate the most popular check-in time during the week for\\\n",
    "# individual cities\n",
    "import get_popular_time\n",
    "# create a bunch of files that record the time adn average check-ins time for each city, but we only record\\\n",
    "# 10 cities with the most number of check-ins\n",
    "get_popular_time.create_popular_time_cities()\n",
    "get_popular_time.plot_popular_time_all_cities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
