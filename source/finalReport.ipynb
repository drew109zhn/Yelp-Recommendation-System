{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 0. Initial set up\n",
    "In this report, we will walk you through our learning process. At the same time, we will also include information about steps to run our code and visualize our results. \n",
    "\n",
    "First, please run the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Background\n",
    "We used data made public for educational purposes by Yelp Inc.. To motivate students and researchers in their pursuits of developments of better algorithms and practices in tackling big and complex data, Yelp created public challenges by publishing a part of their data and  challeng scholars and curious minds to find meanings in their wealth of data. The data we downloaded from Yelp is 4.98GB; we used data from round 9 challenge. The link to the data is: https://www.yelp.com/dataset_challenge\n",
    "\n",
    "\n",
    "# 1. Item-based recommendation system\n",
    "# 2. Our original recommendation method\n",
    "# 3. Testing the results\n",
    "## IV. Results\n",
    "## V. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Aims\n",
    "First, we want to do some data exploration to find interesting patterns from the data. Some interesting questions that we wanted to answer are:\n",
    "1, What are the times in a week where most people would check in to businesses on Yelp?\n",
    "2, How does the distribution of users' number of reviews look like?\n",
    "3, How does the distribution of users' ratings look like? \n",
    "4, What constitute businesses on Yelp?\n",
    "5, Are there any correlations between Yelp users' number of friends and their average ratings?\n",
    "\n",
    "Second, we decided to apply what we learned in this data science course to  create a system that recommends businesses to Yelp users. In particular, given that Yelp users may have reviewed businesses that they have experienced, we used the machine learning methods introduced in class to find out -- from a poll of potential businesses close to where the users are -- the businesses that they may like, and recommend those to the users. Our recommendation system has restrictions, which will be discussed later in our report.\n",
    "\n",
    "In order to evaluate our models, we also implemented a tradition recommendation method: Item- Based recommendation Algorithm. We did analysis to measure how much our models' recommendations results differ from the Item-Based alogrithm's results. We also analyzed the predictive power of our model and Item-Based model -- given our data-- by the conventional training-testing approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methods\n",
    "# 1, Obtain and organize data:\n",
    "- First, you have to download the Yelp data from https://www.yelp.com/dataset_challenge/dataset. You will find a compressed file in your Downloads forlder. Uncompress it (tar xvzf yelp_dataset_challenge_round9.tar), name it 'yelp_data', and move the resulting folder in the directory of this project (It has to be in the same directory as the source folder, i.e. not inside the source folder).\n",
    "\n",
    "- Second, install dependency for this project: \n",
    "        pip install psycopg2 \n",
    "        pip install simplejson\n",
    "\n",
    "- Third, create the Yelp data based from the json files downloaded from Yelp. Into your terminal, inside this project folder: \n",
    "        createdb yelp\n",
    "  cd into source folder, on your terminal: \n",
    "        psql yelp -f create_yelp_dataset.sql\n",
    "  The schema of this data base are all written inside create_yelp_dataset.sql. We made some modifications to the general structure of data base given from Yelp DUY CAN YOU WRITE THIS PART\n",
    "  Into the terminal: \n",
    "      python populate_db.py\n",
    "  We do not make the code live in this report (functions inside populate_db are care called from this report), because this code should be run once only, and you should not make a mistake of running it again. The time it takes to import data from json files to local database is 40 mins- 60 mins. \n",
    "  \n",
    "- (Optional) If you prefer to deal with .csv files, you can convert the json files to csv files\n",
    "        \n",
    "        python json_to_csv_converter.py json_file_name\n",
    "   This actually does not help in anything in our project, except wasting your time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2, Set up input files\n",
    "Our implementation requires reading and writing to files to communicate between differnt processes of the algorithms. \n",
    "First, create necessary folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up all necessary directories\n",
    "import setup_io\n",
    "setup_io.create_output_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside our project folder, we created:\n",
    "- ./output : Where all outputs are stored\n",
    "- ./output/users: We need to find users' id that have reviewed a certain number of businesses in order to run our recommendation system. All these users ids will be quieried and ordered in order of descending users' number of reviews. The ids' are then written into files inside this folder\n",
    "- ./output/original: Our algorithm requires that given an user's id, we have to extract information about attributes of all busnissess that the user experienced, and whether the user rated this business positively or negatively. Each user's data will split into training data and test data. All files used to run our original system will be stored inside this folder. Details of each file are explained later.\n",
    "- ./output/item_based: Item- based recommendation algorithm requires inputs of user_id, businesses_id and users' ratings of the busineeses. We also divide each user's data into train data file and test data file. All files necessary to run this algorithm are stored inside this folder. Details of each file will be explained later.\n",
    "- ./output/potentials: For each user, we queried businesses' id in the same cities as the businesses that the user has reviewed. These businesses are potential recommendations for the users. The two recommendation systems will predict -- from this pool of potential recommendations -- the businesses that the user will more likely to love. Data of potential recommendations, and cities that the users have been to, are all stored inside this folder. Files details will be explained later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import get_ideal_users as users\n",
    "# Get user id that have more than 1000 reviews\n",
    "users.get_more_1000_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import get_ideal_users as users\n",
    "import macros as m\n",
    "import get_reviews_attributes as process_input\n",
    "# Get the first x users that have the most reviews\n",
    "x = 100 # Write your desired number of users to run analysis here, default is 100\n",
    "fname = m.user_limit + str(x) + \".txt\"\n",
    "users.get_users_limit(x, fname)\n",
    "#training percent: 70\n",
    "process_input.process_all_user_input(fname, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import get_reviews_attributes as process_input\n",
    "process_input.process_one_user_input('CxDOIDnH8gp9KXzpBHJYXw', 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to create such files and plots to demonstrate the most popular check-in time during the week for\\\n",
    "# individual cities\n",
    "import get_popular_time\n",
    "# create a bunch of files that record the time adn average check-ins time for each city, but we only record\\\n",
    "# 10 cities with the most number of check-ins\n",
    "get_popular_time.create_popular_time_cities()\n",
    "get_popular_time.plot_popular_time_all_cities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import restaurant_location_ratings\n",
    "#restaurant_location_ratings.create_res_loc_ratings_by_city()\n",
    "#restaurant_location_ratings.plot_res_loc_ratings_by_city()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import visualize_friends_stars_users as f_r\n",
    "f_r.get_friends_ratings_friends()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}